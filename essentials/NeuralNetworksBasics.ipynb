{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Neural Network From Scratch\n",
    "\n",
    "To recognize individual digits we will use a three-layer neural network:\n",
    "\n",
    "![neural-network-for-digits](../images/neural-network-for-digits.PNG)\n",
    "\n",
    "The input layer of the network contains neurons encoding the values of the input pixels. Our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784=28×28 neurons. The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey.\n",
    "\n",
    "The second layer of the network is a hidden layer. We denote the number of neurons in this hidden layer by n, and we'll experiment with different values for n. The example shown illustrates a small hidden layer, containing just n=15 neurons.\n",
    "\n",
    "The output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output ≈ 1, then that will indicate that the network thinks the digit is a 0. If the second neuron fires then that will indicate that the network thinks the digit is a 1. And so on. A little more precisely, we number the output neurons from 0 through 9, and figure out which neuron has the highest activation value. If that neuron is, say, neuron number 6, then our network will guess that the input digit was a 6. And so on for the other output neurons.\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "## Learning with Gradient Descent\n",
    "\n",
    "We'll use the notation $\\vec{x}^{\\,}$ to denote a training input. It'll be convenient to regard each training input $\\vec{x}^{\\,}$ as a 28×28=784 - dimensional vector. Each entry in the vector represents the grey value for a single pixel in the image. The desired output will be $\\vec{y}^{\\,}(\\vec{x}^{\\,})$ where $\\vec{y}^{\\,}$ is a 10 - dimensional vector. For instance, if $\\vec{x}^{\\,}$ is a 28x28=784 picture representing 7, then the ideal output of the network should be  $\\vec{y}^{\\,}(\\vec{x}^{\\,}) = [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]^T$\n",
    "\n",
    "What we'd like is an algorithm that lets us find weights and biases so that the output from the network is as close as possible to the ideal output $\\vec{y}^{\\,}(\\vec{x}^{\\,})$ for all training inputs $\\vec{x}^{\\,}$. To quantify how well we're achieving this goal we define a **cost function**. The cost function should output a high value (scalar) if our network is not sure how to recognize digits and low value when the network is confident that it can recognize digits correctly.\n",
    "\n",
    "$$C(\\vec{w}^{\\,},\\vec{b}^{\\,}) = \\frac{1}{2n}\\sum_x{||\\vec{y}^{\\,}(\\vec{x}^{\\,}) - \\vec{a}^{\\,}(\\vec{x}^{\\,},\\vec{w}^{\\,},\\vec{b}^{\\,})||^2}$$\n",
    "\n",
    "The function above can be our cost function for this example. In this function, $\\vec{w}^{\\,}$ represents all weights in the network, $\\vec{b}^{\\,}$ represents all biases in the network, $\\vec{a}^{\\,}$ is the vector of outputs from the network when $\\vec{x}^{\\,}$ is input, and the sum is over all training inputs, $\\vec{x}^{\\,}$. We'll call $C$ _the quadratic cost function_; it's also sometimes known as the _mean squared error_ or just **MSE**.\n",
    "\n",
    "The cost $C(\\vec{w}^{\\,},\\vec{b}^{\\,})$ becomes small, i.e., $C(\\vec{w}^{\\,},\\vec{b}^{\\,})≈0$, precisely when $\\vec{y}^{\\,}(\\vec{x}^{\\,})$ is approximately equal to the output, $\\vec{a}^{\\,}$, for all training inputs, $\\vec{x}^{\\,}$. So our training algorithm has done a good job if it can find weights and biases so that $C(\\vec{w}^{\\,},\\vec{b}^{\\,})≈0$. By contrast, it's not doing so well when $C(\\vec{w}^{\\,},\\vec{b}^{\\,})$ is large - that would mean that $\\vec{y}^{\\,}(\\vec{x}^{\\,})$ is not close to the output $\\vec{a}^{\\,}$ for a large number of inputs. So the aim of our training algorithm will be to minimize the cost $C(\\vec{w}^{\\,},\\vec{b}^{\\,})$ as a function of the weights and biases. In other words, we want to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as **gradient descent**.\n",
    "\n",
    "To explain the intuition behind gradient descent we will start with the function $C(x,y) = x^2 + y^2$. The gradient is a vector of all partial derivatives of a function:\n",
    "\n",
    "$$\\nabla C(x,y) = [\\frac{\\delta C(x,y)}{\\delta x}, \\frac{\\delta C(x,y)}{\\delta y}]$$\n",
    "\n",
    "When we calculate the gradient we get:\n",
    "\n",
    "$$\\nabla C(x,y) = [2x, 2y]$$\n",
    "\n",
    "What does the gradient tell us? When we are at the point $(x,y)$ it explains in which **DIRECTION (not value)** $(2x, 2y)$ we need to go in order to maximize the impact on the function $C(x, y)$. Hence, when we take the negative gradient we are going in a direction to minimize the loss/cost function.\n",
    "\n",
    "How does this translate to our neural network? Well, our cost function $C(\\vec{w}^{\\,},\\vec{b}^{\\,})$ has a lot of parameters but in the end, it's just a multivariable function. We can randomly choose $\\vec{w}^{\\,}$ and $\\vec{b}^{\\,}$ and calculate the gradient at that point of a cost function. The gradient will tell us in which direction we should go in order to reduce the value of a cost function. In order to go downhill, we will need some learning step that will represent how fast we want to go downhill. If steps are small, then learning process will be slow. If steps are high, then we might never find a minimum.\n",
    "\n",
    "The gradient is actually averaged over all training examples. This means that we calculate gradient for one training example $\\vec{x_1}^{\\,}$ and note what changes of $\\vec{w}^{\\,}$ and $\\vec{b}^{\\,}$ should be in order to improve the cost function for this example. We need to do this for every training example and find the average value of individual changes in order to make a new step in the cost function. This process is costly. So, we are going to use **stochastic gradient descent** that will take batches of input training data and make gradient descent based on those batches (we are essentially making an approximation of a true gradient using this batch approach). Of course, the estimate won't be perfect - there will be statistical fluctuations - but it doesn't need to be perfect: all we really care about is moving in a general direction that will help decrease C, and that means we don't need an exact computation of the gradient. In practice, stochastic gradient descent is a commonly used and powerful technique for learning in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Neural Network From Scratch\n",
    "\n",
    "We are going to use sigmoid neurons in this neural network. To calculate activations in the second layer of the network we use:\n",
    "\n",
    "$$a_2 = \\sigma(w*a_1 + b)$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $w$ is a matrix of weights between first and the second layer of a network\n",
    "- $a_1$ is a vector of activations in the input layer\n",
    "- $b$ is a vector of biases for the sigmoid neurons in the second layer\n",
    "\n",
    "For instance, if our network has 10 neurons in the input layer 7 neurons in the second layer, and 5 neurons in the output layer (a 3 layered network) then to calculate activations in the second layer we use:\n",
    "\n",
    "$$a'_{7x1} = \\sigma(w_{7x10}a_{10x1} + bias_{7x1})$$\n",
    "\n",
    "Finally, for the output layer we have:\n",
    "\n",
    "$$a''_{5x1} = \\sigma(w_{5x7}a'_{7x1} + bias_{5x1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    This class encapsulates neural network mechanics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        NeuralNetwork constructor.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        sizes: array that represents number of neurons in different layers of the network\n",
    "        \"\"\"\n",
    "        self.number_of_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]\n",
    "        \n",
    "        \n",
    "    def learn(self, training_data, epochs, batch_size, learning_rate, test_data=None):\n",
    "        \"\"\"\n",
    "        Performs a neural network learning process.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        training_data: training data for the neural network in the form (x, y), \n",
    "        representing the training inputs and the desired outputs\n",
    "        \n",
    "        epochs: number of epochs to train the neural network\n",
    "        batch_size: size of the mini batch\n",
    "        learning_rate: learning rate for the steepest descent\n",
    "        test_data: if available it is used to see how well the network is learning at each epoch\n",
    "        \"\"\"\n",
    "        if test_data: size_test = len(test_data)\n",
    "        size_training = len(training_data)\n",
    "        \n",
    "        # train a net for a specified number of epochs\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batch_size] for k in range(0, size_training, batch_size)]\n",
    "            \n",
    "            # do SGD per batch\n",
    "            for batch in batches:\n",
    "                self._sgd_step(batch, learning_rate)\n",
    "            \n",
    "            # show the results of a step if test data is available\n",
    "            if test_data:\n",
    "                print(f\"Epoch {i}: {self.evaluate(test_data)} / {size_test}\")\n",
    "            # otherwise, just notify that epoch is completed\n",
    "            else:\n",
    "                print(f\"Epoch {i} complete\")\n",
    "                \n",
    "                \n",
    "    def _sgd_step(self, batch, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one gradient descent step for the batch.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        batch: subset of the training data in the form (x,y)\n",
    "        learning_rate: learning rate for the steepest descent\n",
    "        \"\"\"\n",
    "        # create empty gradients\n",
    "        nabla_w = [np.zeros(w_matrix.shape) for w_matrix in self.weights]\n",
    "        nabla_b = [np.zeros(b_vector.shape) for b_vector in self.biases]\n",
    "        \n",
    "        # for each data point in the batch \n",
    "        # update the gradient according to the backpropagation algorithm\n",
    "        for x,y in batch:\n",
    "            delta_nabla_b, delta_nabla_w = self._backpropagation(x, y)\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            \n",
    "        # finally update the parameters of the network (weights and biases)\n",
    "        self.weights = [w - (learning_rate/len(batch))*nw for w,nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (learning_rate/len(batch))*nb for b,nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "    def _backpropagation(self, x, y):\n",
    "        \"\"\"\n",
    "        This function executes a backpropagation algorithm for one training example.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        x: activations in the input layer of a neural network\n",
    "        y: desired output of a neural network\n",
    "        \"\"\"\n",
    "        # gradient of weights and biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        # feedforward pass\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        \n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            activations.append(self._sigmoid(z))\n",
    "            zs.append(z)\n",
    "        \n",
    "        # backpropagation pass\n",
    "        print(zs[-1], self._cost_derivative(activations[-1], y))\n",
    "        \n",
    "        delta = self._cost_derivative(activations[-1], y)*self._sigmoid_derivative(zs[-1])\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        for l in range(2, self.number_of_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self._sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            print(delta.shape, activations[-l-1].shape)\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_w, nabla_b)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of a neural network by feeding forward information \n",
    "        from the input layer to the output layer through sigmoid neurons.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        x: activations in the input layer of a neural network. They should be represented in the form of a vector (n, 1)\n",
    "        \"\"\"\n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            x = self._sigmoid(np.dot(w, x) + b) \n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Calculates sigmoid function value.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        z: input value for a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def _sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Calculates sigmoid derivative function value.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        z: input value for a sigmoid derivative function\n",
    "        \"\"\"\n",
    "        return self._sigmoid(z)*(1 - self._sigmoid(z))\n",
    "    \n",
    "    def _cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Vector of partial derivatives of the loss function \n",
    "        with respect to activations in the output layer.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        output_activations: activations in the output layer\n",
    "        y: desired output of a neural network\n",
    "        \"\"\"\n",
    "        return output_activations - y\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "# Notes:\n",
    "# - when z is a vector (in sigmoid function) function will return vector where sigmoid function is applied element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNetwork = NeuralNetwork([100, 50, 20, 10])\n",
    "# neuralNetwork.learn() - this is how to init learning in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't start the network to learn digits because data preparation would take some time to program. The intention was to get a deep understanding of how the neural network learns from training examples.\n",
    "                                                                                                                                                                                                                                      \n",
    "                                                                                                                                                                                                                                      12:00 AM September 9, 2020"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
