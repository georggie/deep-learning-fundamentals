{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "\n",
    "We will try to explain how someone should go about implementing a neural network from scratch. The idea of implementation is taken from this [book](http://neuralnetworksanddeeplearning.com/chap1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports of dependencies\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the constructor of a neural network, we store weights and biases of each layer and the number of layers. The constructor accepts one argument and that is an array explaining the number of neurons in each layer of a network.\n",
    "\n",
    "```\n",
    ">>> neuralNetwork = NeuralNetwork([10, 7, 5])\n",
    "```\n",
    "\n",
    "As a result, we've created a neural network with 10 neurons in the input layer, 7 neurons in the first hidden layer, and 5 neurons in the output layer.\n",
    "\n",
    "```\n",
    ">>> neuralNetwork.weights = [matrix(7x10), matrix(5x7)]\n",
    ">>> neuralNetwork.biases = [vector(7), vector(5)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    This class encapsulates neural network mechanics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        NeuralNetwork constructor.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        sizes: array that represents number of neurons in different layers of the network\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.number_of_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weights: (7, 10)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Weights: (5, 7)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Biases: (7, 1)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Biases: (5, 1)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neuralNetwork = NeuralNetwork([10, 7, 5])\n",
    "\n",
    "for w in neuralNetwork.weights:\n",
    "    display(f'Weights: {w.shape}')\n",
    "    \n",
    "for b in neuralNetwork.biases:\n",
    "    display(f'Biases: {b.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the SGD(Stochastic Gradient Descent) - learning function besides training data and the number of epochs to train accepts the size of the batch and the learning rate. For each epoch, we first shuffle the data and split them into mini-batches. For each mini-batch, we perform a step in gradient descent. In each epoch, we evaluate how much our metric (accuracy) changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    This class encapsulates neural network mechanics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        NeuralNetwork constructor.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        sizes: array that represents number of neurons in different layers of the network\n",
    "        \"\"\"\n",
    "        self.sizes = sizes\n",
    "        self.number_of_layers = len(sizes)\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]\n",
    "        \n",
    "        \n",
    "    def learn(self, training_data, epochs, batch_size, learning_rate, test_data=None):\n",
    "        \"\"\"\n",
    "        Performs a neural network learning process.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        training_data: training data for the neural network in the form (x, y), \n",
    "        representing the training inputs and the desired outputs\n",
    "        epochs: number of epochs to train the neural network\n",
    "        batch_size: size of the mini batch\n",
    "        learning_rate: learning rate for the steepest descent\n",
    "        test_data: if available it is used to see how well the network is learning at each epoch\n",
    "        \"\"\"\n",
    "        if test_data: \n",
    "            size_test = len(test_data)\n",
    "            \n",
    "        size_training = len(training_data)\n",
    "        \n",
    "        # train a net for a specified number of epochs\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batch_size] for k in range(0, size_training, batch_size)]\n",
    "            \n",
    "            # do SGD per batch\n",
    "            for batch in batches:\n",
    "                self._sgd_step(batch, learning_rate)\n",
    "            \n",
    "            # show the results of a step if test data is available\n",
    "            if test_data:\n",
    "                print(f\"Epoch {i}: {self.evaluate(test_data)} / {size_test}\")\n",
    "            # otherwise, just notify that epoch is completed\n",
    "            else:\n",
    "                print(f\"Epoch {i} complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform an SGD step we calculate gradients for each data point in the batch. Finally, in the last step, we average all gradient vectors and update weights w and biases b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _sgd_step(self, batch, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one gradient descent step for the batch.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        batch: subset of the training data in the form (x,y)\n",
    "        learning_rate: learning rate for the steepest descent\n",
    "        \"\"\"\n",
    "        # create empty gradients\n",
    "        nabla_w = [np.zeros(w_matrix.shape) for w_matrix in self.weights]\n",
    "        nabla_b = [np.zeros(b_vector.shape) for b_vector in self.biases]\n",
    "        \n",
    "        # for each data point in the batch \n",
    "        # update the gradient according to the backpropagation algorithm\n",
    "        for x,y in batch:\n",
    "            delta_nabla_b, delta_nabla_w = self._backpropagation(x, y)\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            \n",
    "        # finally update the parameters of the network (weights and biases)\n",
    "        self.weights = [w - (learning_rate/len(batch))*nw for w,nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (learning_rate/len(batch))*nb for b,nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most important function in the class. It accepts a $x$ vector that represents activations in the input layer for the network and $y$ vector that represents the ideal output of the network for given $x$. \n",
    "\n",
    "We want to track activations and `z` values of each neuron in the network. In order to do that we need a forward pass through the network. Then we just follow the formulas from the backpropagation algorithm derivation to obtain gradients for the current input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _backpropagation(self, x, y):\n",
    "        \"\"\"\n",
    "        This function executes a backpropagation algorithm for one training example.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        x: activations in the input layer of a neural network\n",
    "        y: desired output of a neural network\n",
    "        \"\"\"\n",
    "        # gradient of weights and biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        # feedforward pass\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        \n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            activations.append(self._sigmoid(z))\n",
    "            zs.append(z)\n",
    "        \n",
    "        # backpropagation pass        \n",
    "        delta = self._cost_derivative(activations[-1], y)*self._sigmoid_derivative(zs[-1])\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        for l in range(2, self.number_of_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self._sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            print(delta.shape, activations[-l-1].shape)\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_w, nabla_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full implementation of the neural network class is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \"\"\"\n",
    "    This class encapsulates neural network mechanics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        NeuralNetwork constructor.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        sizes: array that represents number of neurons in different layers of the network\n",
    "        \"\"\"\n",
    "        self.number_of_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.biases = [np.random.randn(x, 1) for x in sizes[1:]]\n",
    "        \n",
    "        \n",
    "    def learn(self, training_data, epochs, batch_size, learning_rate, test_data=None):\n",
    "        \"\"\"\n",
    "        Performs a neural network learning process.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        training_data: training data for the neural network in the form (x, y), \n",
    "        representing the training inputs and the desired outputs\n",
    "        \n",
    "        epochs: number of epochs to train the neural network\n",
    "        batch_size: size of the mini batch\n",
    "        learning_rate: learning rate for the steepest descent\n",
    "        test_data: if available it is used to see how well the network is learning at each epoch\n",
    "        \"\"\"\n",
    "        if test_data: size_test = len(test_data)\n",
    "        size_training = len(training_data)\n",
    "        \n",
    "        # train a net for a specified number of epochs\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            batches = [training_data[k:k+batch_size] for k in range(0, size_training, batch_size)]\n",
    "            \n",
    "            # do SGD per batch\n",
    "            for batch in batches:\n",
    "                self._sgd_step(batch, learning_rate)\n",
    "            \n",
    "            # show the results of a step if test data is available\n",
    "            if test_data:\n",
    "                print(f\"Epoch {i}: {self.evaluate(test_data)} / {size_test}\")\n",
    "            # otherwise, just notify that epoch is completed\n",
    "            else:\n",
    "                print(f\"Epoch {i} complete\")\n",
    "                \n",
    "                \n",
    "    def _sgd_step(self, batch, learning_rate):\n",
    "        \"\"\"\n",
    "        Performs one gradient descent step for the batch.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        batch: subset of the training data in the form (x,y)\n",
    "        learning_rate: learning rate for the steepest descent\n",
    "        \"\"\"\n",
    "        # create empty gradients\n",
    "        nabla_w = [np.zeros(w_matrix.shape) for w_matrix in self.weights]\n",
    "        nabla_b = [np.zeros(b_vector.shape) for b_vector in self.biases]\n",
    "        \n",
    "        # for each data point in the batch \n",
    "        # update the gradient according to the backpropagation algorithm\n",
    "        for x,y in batch:\n",
    "            delta_nabla_b, delta_nabla_w = self._backpropagation(x, y)\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            \n",
    "        # finally update the parameters of the network (weights and biases)\n",
    "        self.weights = [w - (learning_rate/len(batch))*nw for w,nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (learning_rate/len(batch))*nb for b,nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "    def _backpropagation(self, x, y):\n",
    "        \"\"\"\n",
    "        This function executes a backpropagation algorithm for one training example.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        x: activations in the input layer of a neural network\n",
    "        y: desired output of a neural network\n",
    "        \"\"\"\n",
    "        # gradient of weights and biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        # feedforward pass\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        \n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, activations[-1]) + b\n",
    "            activations.append(self._sigmoid(z))\n",
    "            zs.append(z)\n",
    "        \n",
    "        # backpropagation pass        \n",
    "        delta = self._cost_derivative(activations[-1], y)*self._sigmoid_derivative(zs[-1])\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        nabla_b[-1] = delta\n",
    "        \n",
    "        for l in range(2, self.number_of_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self._sigmoid_derivative(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            print(delta.shape, activations[-l-1].shape)\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return (nabla_w, nabla_b)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _feedforward(self, x):\n",
    "        \"\"\"\n",
    "        Calculates the output of a neural network by feeding forward information \n",
    "        from the input layer to the output layer through sigmoid neurons.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        x: activations in the input layer of a neural network. They should be represented in the form of a vector (n, 1)\n",
    "        \"\"\"\n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            x = self._sigmoid(np.dot(w, x) + b) \n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Calculates sigmoid function value.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        z: input value for a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def _sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Calculates sigmoid derivative function value.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        z: input value for a sigmoid derivative function\n",
    "        \"\"\"\n",
    "        return self._sigmoid(z)*(1 - self._sigmoid(z))\n",
    "    \n",
    "    def _cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Vector of partial derivatives of the loss function \n",
    "        with respect to activations in the output layer.\n",
    "        \n",
    "        Attributes:\n",
    "        -----------\n",
    "        output_activations: activations in the output layer\n",
    "        y: desired output of a neural network\n",
    "        \"\"\"\n",
    "        return output_activations - y\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "# Notes:\n",
    "# - when z is a vector (in sigmoid function) function will return vector where sigmoid function is applied element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNetwork = NeuralNetwork([100, 50, 20, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't start the network to learn digits because data preparation would take some time to program. The intention was to get a deep understanding of how the neural network learns from training examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
